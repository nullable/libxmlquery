\documentclass[a4paper]{report}

%============ Packages ================
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{framed}
\usepackage[pdfborder={0 0 0}]{hyperref}
%============ End Packages =============

%============ Title ================
\title{\textbf{\Huge{libxmlquery}}\\A lightweight library to query and manipulate XML files}
\author{
Vasco Fernandes\\
MEIC-T, 53801\\
vasco.fernandes@ist.utl.pt \\
\and
Frederico Gon\c{c}alves\\
MEIC-T, 57843\\
frederico.goncalves@ist.utl.pt\\
}
%============ End Title =============

%============ Document ============
\begin{document}

\maketitle

\tableofcontents

%Intro%
\chapter{Introduction}

\section{Motivation} %O google taduz domotica para Home Automation.....%
	As computers tend to get smaller and powerful, ubiquitous environments begin to emerge as possible and reliable systems. One of the main characteristics of an ubiquitous environment is the fact that it's filled with
	devices that interoperate in order to provide the user with different kind of services. Home Automation is part of the ubiquitous computing philosophy, in the sense that it fills a certain space with devices that work together in
	order to provide users with services that are home oriented. Each device has its own characteristics, like a certain operating system, communication medium (Bluetooth, Wifi, ethernet, etc.), communication protocols and
	computational 	capacity. This creates a problem in terms of communication between devices, because each one will communicate with different methods and rules for treating data. This problem stands even if the devices
	are of equal characteristics, because there are several different software and hardware configurations that can have an effect on data transmission and handling.

	We need a system that provides communication between devices and lets us handle the transmitted data with great ease.

\section{Problem description}
	When targeting systems built for Home Automation and as said before, we expect a highly heterogeneous environment. One possible solution for this problem is to agree in a common representation and treating method
	for transmitted data.

	Such can be achieved by using XML (e\textbf{X}tensible \textbf{M}arkup \textbf{L}anguage). There are several solutions that use this approach, however they tend to be implemented in languages not suitable for
	embedded systems. Such solutions also provide a mechanism to query the XML documents, but yet again they're too heavy for embedded systems. We need a lightweight solution that parses and queries XML
	documents with good degree of performance in embedded systems.

	Another concern is how easily is to develop some program that parses and manipulates XML. There are three different phases in this kind of program:

	\begin{enumerate}
		\item \textbf{Parsing} - Reading an XML document and building a data structure to hold the parsed information.
		\item \textbf{Getting the correct data} - Traverse the data structure and retrieve the correct portion of the XML that needs to be manipulated.
		\item \textbf{Manipulating} - Alter the retrieved structure and save the changes.
	\end{enumerate}

	Each of each phase has different levels of complexity but probably the second is the most tiresome. If the right querying engine is provided to gather the data from the data structure, then this phase can actually be
	speed upped. Also, with the right techniques, parsing can be done efficiently without the need to write code to parse different XML documents.

\section{Goal}
	The goal is simple, build a lightweight mechanism to parse, query and manipulate XML documents. To achieve this goal, we've built a library called libxmlquery.

\section{What is libxmlquery?}
	Libxmlquery is a library designed to parse XML files and query them. It also allows manipulation of the xml elements. It is written in C and was tested in three different kind of operating systems - MS-Windows 7, Linux 2.6
	and MacOSx 10.4. It uses the flex library to implement the parser, but it is not compiled against it, which means that the end system doesn't need to have the flex library. However, it has a strong dependency on the C library
	and won't work on systems that don't have it.\\


	The rest of this report is organized as follows: in chapter \ref{chap:arch} we outline the design of our architecture, present the main modules (sections \ref{sec:dom}, \ref{sec:parser} and \ref{sec:sels}) and explain the data
	structure and macros used in them (section \ref{sec:helpers}); in chapter \ref{chap:eval} we present some profiling tests to the memory usage (section \ref{sec:mprof}) and time usage \ref{sec:tprof}; chapter
	\ref{chap:concl} we present our main conclusions; in the appendix we shows how to compile against our lib (appendix \ref{chap:gcc}), some sample applications (appendix \ref{chap:app}) and the entire libxmlquery
	documentation (appendix \ref{chap:doc}).
%END Intro%

%Arch%
\chapter{Architecture}\label{chap:arch}
\section{Overview}\label{sec:overview}
	We begin by describing the modules of our implementation. Figure \ref{fig:arch} shows the scheme of libxmlquery.

	%Figura da arquitectura
	 \begin{figure}[h!]
		\centering
		\label{fig:arch}
		\includegraphics[width=0.5\textwidth] {arch}
		\caption{libxmlquery module design. Three main modules - DOM module responsible for implementing the data structure that holds the xml contents; Parser module, which parses queries and xml documents;
		Querying engine module, which enables querying to a dom structure.}
	 \end{figure}

	The DOM module defines structures and functions to manipulate a given XML document. As shown in figure \ref{fig:arch}, the DOM module doesn't depend on any other module so it doesn't have to change if any of the others changes. However, if modifications are made to it, then the other modules must change as well, unless the function signature stays exactly the same.

	The Parser module is responsible for parsing queries and XML documents. Due to implementation details (see section \ref{sec:parser}) we weren't able to separate query parsing from XML parsing. This means that the parser depends on both querying engine module and DOM module to work correctly. If any of them changes the parser has to be changed as well. However, if a new parser is needed instead of the one we use, the change wouldn't affect the DOM module and the querying engine module would only need one line changed (The line that calls the function, which parses a query). This new parser would only have to build each query	and DOM according to each module's interface.

	Finally, the query engine module is responsible for evaluating query plans and querying XML documents. Each query returns a list of the selected elements. Notice that this module depends on both the DOM module and parser module, because it needs to parse queries and use the data structure provided by the DOM module to search the XML document. In a normal use, one would call the query engine module, which would call the parser module and then the DOM module.

	We've adopted this architecture in order to support flexibility and modularity. However, the user may need to interact directly with the DOM module, which can be achieve by calling the functions provided by its interface. This is also true for the parser module.

	In order to ease the continued development of the library, we've structured the files on disk according to figure \ref{fig:disk}.

	%Figura das pastas em disco.
	 \begin{figure}[h!]
		\centering
		\label{fig:disk}
		\includegraphics[width=0.30\textwidth] {disk}
		\caption{libxmlquery organization on disk.}
	 \end{figure}

	The first two folders are pretty much descriptive, the third one is where all the query engine source code is. The data\_structures folder has various data structures to help us (see section \ref{sec:helpers}) and the includes
	folder has all the header files of our library.

	The next sections details each module's implementation.

\section{The Document Object Model Module}\label{sec:dom}
	This section describes how our implementation of DOM works. Although DOM is a standard from W3C\footnote{\url{http://www.w3.org/} - Official site of the World Wide Web consortium} our implementation does not compatibility with this recomendation, in order to keep it simple and smaller enough for embedded systems. We use DOM as it is the stardard nomenclature when refering to abstract document structures. The goal of our DOM implementation is to keep in memory the information stored in the XML file. The idea is to keep the same semantics offered by the
	XML. In order to do this, the DOM creates a tree where every node is either an XML element, attribute, normal text, or CDATA element. Each time an XML document is parsed, the DOM module returns a pointer to its corresponding	tree root.

	\pagebreak
	A document root is just a structure that holds two pointers. The first one points to what we call XML declaration and the second points to the root element of the XML document. For the sake of better understanding to which structure we're referring to, we'll refer to a document root as a DOM tree. As an example consider the following XML document	:

	\lstset{language=XML,caption=Sample XML file, captionpos=b}
	\begin{lstlisting}
		<?xml version="1.0" ?>
		<a>
		   <b />
		</a>
	\end{lstlisting}

	The DOM tree returned by the module for this XML contains in its XML declaration pointer the tree corresponding to:
	\lstset{language=XML,caption=Sample XML declaration, captionpos=b}
	\begin{lstlisting}
		<?xml version="1.0" ?>
	\end{lstlisting}

	and in its second pointer will have the tree corresponding to:
	\label{lst:xml_example}
	\lstset{language=XML,caption=Sample XML file without XML declaration, captionpos=b}
	\begin{lstlisting}
		<a>
		   <b />
		</a>
	\end{lstlisting}

	Because some documents may not contain the XML declaration part, the DOM tree will have that pointer pointing to NULL in such cases.

	\subsection{Building the Document Object Module}
		As explained before, the parser module is responsible for constructing the tree returned by the DOM module. It does this by calling upon functions provided by the DOM module. Currently, the simplest way of building a DOM tree is to create a new document with the appropriate function and iteratively add all the elements, attributes, normal text and CDATA present in the XML (These functions are described in appendix	\ref{chap:doc}).

		As a simple example, suppose we would like to generate the DOM structure for the XML in listing \ref{lst:xml_example}. The following code shows how to do this:

	\begin{center}
	\lstset{language=C,numbers=left, captionpos=b, caption=Sample code to build a DOM tree.}
		\begin{tabular}{c}
	\begin{lstlisting}
...
doc* document = new_document(NULL);
set_doc_root(document, new_element_node("a"));

dom_node* root = get_doc_root(document);
append_child(root, new_element_node("b"));
...
	\end{lstlisting}
	\end{tabular}
	\end{center}

		Line 2 creates a DOM tree with no XML declaration. On line 3 we make the root of this tree point to element \textbf{a}. Lines 5 and 6 add element \textbf{b} to the DOM tree and complete the building process.

	\subsection{Handling and viewing DOM trees}
		The DOM module also provides functions to retrieve and manipulate the various DOM trees in memory (see appendix \ref{chap:doc}). We strongly recommend the use of \emph{getters} and \emph{setters} to do this, because they deal with implementation details that shouldn't be dealt by the user.

		As the DOM tree gets modified it might help the user if he or she could actually see how it looks like, or save it to disk. This is where serialization comes into play. Currently we support three kinds of serialization - XML;
		JSON (\textbf{J}ava\textbf{S}cript \textbf{O}bject \textbf{N}otation); YAML (\textbf{Y}AML \textbf{A}in't \textbf{M}arkup \textbf{L}anguage). Serialization is the reverse of parsing, as in it transforms the DOM tree to a string.

	\subsection{Cleaning a document}
		Since our implementation is done in C, there is no garbage collection mechanism. This means that the DOM tree must be freed manually. We provide two functions to do this. One destroys the entire DOM tree the other destroys a generic DOM node (a DOM tree node). However, the second function must be used with care, nodes are not independent of their positions in the DOM, destroying one node will also destroy its children. Freeing up memory space is crucial in an embedded system, but can have a great impact in the system's performance. We leave it to the user to decide where to call these functions.

\section{The Parser Module}\label{sec:parser}
	The parser module implements both the XML parser and the query parser. The implementation of both parsers uses flex lexical analyzer\footnote{\url{http://flex.sourceforge.net/} - Flex home page.} and the bison parser generator\footnote{\url{http://dinosaur.compilertools.net/bison/index.html} - Bison site} they provide a simple, flexible and extendable way to build very powerful parsers. Moreover, the outcome is a fully optimized parser.. One of the biggest disadvantages of flex and bison is that they rely heavily in global variables to build and run the parser, this means, for simplicity sake, we only have one parser in our library.

	Another disadvantage is the fact that we need global variables to temporally store the parsers result. In order to avoid name clashes, each parser global variable has the prefix \textbf{lxq\_} and shouldn't be used directly. The usage of global variables makes the parser particularly thread unsafe, concurrent threads that need to do parsing should be carefully thoughtout. The call to the actual parsing functions \textbf{must} be mutualy exclusive.

	Flex and Bison are tools that generate parsing code according to a configuration file, this code has no external dependencies.

	\subsection{XML parsing}
		The XML parser implements most of XML features. It is able to parse declarations, elements, attributes, normal text and CDATA regions, at the moment of this paper there was no support for DTD declarations. However, it relies on the fact that the XML had to be validated before parsing began. Since the input is expected to be validated beforehand, the parser can afford to be liberal on what it accepts. Multiple independent xml nodes maybe parsed, there are no limitations on the characters used inside text elements and attributes (other than the obvious '<' character and the '\\0' character respectively).

		Currently we provide two ways of parsing an XML - from a file and from a string - both use the same parsing rules. In fact, there is no difference in them besides the input type.

		In order to build the DOM tree the parser needs to interact with it. For this reason, if the DOM module interface changes the parser will have to be modified as well. (Substituting the DOM module is possible without altering the Parser module if the current interface is respected.)

	\subsection{Query parsing}
	    The query parser implements a parser built for queries compliant to most of the CSS3\footnote{\url{http://www.w3.org/TR/css3-selectors/\#selectors} - CSS3 selector list.} standard selectors. We didn't implement every selector because their are HTML (\textbf{H}yper\textbf{T}ext \textbf{M}arkup \textbf{L}anguage) specific ones that make little sense in xml. For instance, the selector \emph{E:link} selects an element based on the fact that it is a source anchor, which can only happen in HTML files.

		Since querying and XML parsing had to be merge onto a single parser, we had to come up with some kind of distinction between a query and an XML file. We've chosen the character \textbf{@} to indicate that it is a query we're parsing. This means that each query is preceded by the character \textbf{@}, if it isn't our library will prepend it. During development we found it to be important as a quick distinctive characterist of query strings in the code. It is, however, not part of the CSS recomendation and it is not mandatory. As an example consider the following code, which retrieves every element named \textbf{room}.

\begin{center}
	\lstset{language=C,numbers=left, captionpos=b, caption=Sample query to a DOM tree.}
		\begin{tabular}{c}
	\begin{lstlisting}
...
doc* document = parse_xml_from_string("<A> <room /> </A>");
list* query_result = query("@room", get_doc_root(document));
...
	\end{lstlisting}
	\end{tabular}
	\end{center}

		Line 2 parses an XML document from the given string and line 3 queries that document. The result will be an empty element with the tag \textbf{room}. Notice how the query was preceded by \textbf{@}. However, we could simply write \textbf{"room"} as a query.

		Like in XML parsing, the query parser must know the interface of the querying engine module in order to construct the query plan. A query plan is just a sequence of filters and operators that the query engine will apply to the XML nodes. Current implementation applies the filters and operators in the order they were given.

\section{The Querying Engine Module}\label{sec:sels}
	This module is responsible for evaluation of the query plan returned by the parser module and its execution. The result of querying an XML document is always a list containing zero or more XML elements. Note that the list will contain only elements, it will not return text nodes, CDATA nodes or attributes. We'll have to access them through the returned nodes.

	The returned list can then be iterated with the proper functions (see section \ref{sec:helpers}).

	One major concern when querying a document is typing the query correctly. Don't forget that the whitespace character is valid in CSS3 and it means \emph{descendants}. For instance, if we want all elements B \emph{descendants} of A we need to write "A B" as a query, or "@A B". Since the whitespace character has special meaning, "@A" is different from "@ A" (this would remove the \emph{root} from the returned list).

	Cleaning up after querying an XML document is very easy. The generated plan is automatically freed by the library, so the user won't have to bother with this part. The only thing that needs to be freed is the returned list of	results. This list contains pointers to the actual nodes in the DOM tree in order to reduce used memory and enable XML manipulation. This means that the user will only need to destroy the list, leaving the content intact (see appendix \ref{chap:doc}).

	\subsection{Implemented features}
		As said before, our querying engine module is CSS3 compliant. However, not every feature of CSS3 was implemented since some of them are HTML specific.

		In our implementation it's possible to query by element name, attribute name and attribute value. As an example consider these five queries:

			\begin{enumerate}
				\item \textbf{"A"} - All elements with name \textbf{A}.\\
				\item \textbf{"A[b]"} - All elements with name \textbf{A}, that have an attribute with name \textbf{b}.\\
				\item \textbf{"A[b='c']"} - All elements with name \textbf{A}, that have an attribute with name \textbf{b} and value \textbf{c}.\\
				\item \textbf{"A:empty"} - All elements with name \textbf{A} and that have no children.\\
				\item \textbf{"A:nth-child(odd)"} - The elements with name A that are in an odd position relative to their siblings (i.e. 1, 3, 5 ...)\
			\end{enumerate}

        Filter results can be modified through operators:

            \begin{enumerate}
                \item \textbf{"A, B"} - All elements with name \textbf{A} and all elements with name \textbf{B}.\\
                \item \textbf{"A B"} - All elements with name \textbf{B} that are descendents of an element with name \textbf{A}.\\
                \item \textbf{"A $>$ B"} - All elements with name \textbf{B} that are children of an element with name \textbf{A}.\\
                \item \textbf{"A $~$ B"} - All elements with name \textbf{B} that are siblings of an element with name \textbf{A}.\\
                \item \textbf{"A + B"} - All elements with name \textbf{B} that are directly preceded by an element with name \textbf{A}.\\
            \end{enumerate}


		These queries are pretty simple, but we've added some new features. It is possible to use regular expressions everywhere by putting the regular expression between slashes (\textbf{"/"}).
		As an example consider these three queries:

			\begin{enumerate}
				\item \textbf{"/A/"} - Query an XML document for all elements with a name containing \textbf{A}.\\
				\item \textbf{"/A/[/b./]"} - Query an XML document for all elements with a name containing \textbf{A} and have an attribute with name containing \textbf{b} followed by another character.\\
				\item \textbf{"/A/[b = /foo$|$bar/]"} - Query an XML document for all elements that begin by \textbf{A}, have an attribute with name \textbf{b} and its value contains either \textbf{foo} or \textbf{bar}.\\
			\end{enumerate}

		The implemented selectors also support  the CSS3 \emph{filters} relative to document navigation. The only \emph{filters} that  we didn't implement are relative to HTML specific markup language.
		These \emph{filters} do not make sense for XML handling. Nevertheless, we added the possibility for the user to create its own \emph{filters}.

		Custom \emph{filters} are very easy to implement. The user defines a function which represents a filter, which means it will receive all XML nodes and should return true for nodes that match the filter.
		Then, registers that function as a custom \emph{pseudo-filter} and that's it. Custom \emph{filters} can receive 0 or more arguments. Check appendix \ref{chap:doc} for more information on these filters.

\section{Data structures and macros}\label{sec:helpers}

	\subsection{Data structures}
		In order to keep the implementation efficient both in time usage and memory usage, we've developed a couple of optimized structures.

		We've built a circular generic list backed up by an array. There are two great features about this list. First it is backed up by an array, which means that pointers to the elements in the list will be in contiguous memory thus speeding up the access. The second great feature is the circular part. Elements in a list can be added and removed as desired, maintaining $\Theta(1)$ for random access, appending, prepending, enqueing, dequeing, pushing and poping. In order to keep a good performance, the list will only grow if necessary. In other words, if the number of elements to keep in the list is, much smaller or, bigger than its capacity, the list will be resized accordingly.

		We've chosen to keep this data structure separated from the rest of the modules because it can hold any type of object, which means it can be used by any code. In particular, it can be used by the user to store its own program data.

		Since any kind of object can be stored in the list there is no way of type checking each element, so the user must know which elements are present in the list or use another feature of our implementation which is typed elements. Typed elements are just elements with an integer that indicates their type. For instance, if a list would contain three different types of elements we would encourage the use of typed elements in order to know which kind of data the user is dealing with, at access time an extra variable is set to the type of the element.

		However, they aren't generic only because they can store any kind of object, but because they implement different policies as well. The user may choose to use the list as a random access list, as a stack, or as a queue. This provides much greater flexibility.

		Whichever policy and type of elements the user chooses to use and store, he or she can always iterate over the results by using the appropriate functions (see appendix \ref{chap:doc}). After using an iterator it must be destroyed in order to prevent memory leaks. This is also true for our generic lists. However, in this case we can only free the containers of the list, because we can't possibly know which type of object is stored in each one. The user must iterate over the list and free each element one by one. Currently the DOM module and querying engine module use these lists.

		Another generic structure we've built is a red black tree. Red black trees are a very good structure when storing unique values that need to be searched. The height of a red black tree is always $\log_2(n)$, where $n$ is the number of nodes in the tree, thus the search is always done in $\Theta(\log_2(n))$. Currently we use this tree when storing attributes for an XML node.

		One concern when developing code that uses this structure is to be careful and don't use the word \emph{RBNIL} for a variable. The red black tree code defines a global variable called RBNIL and uses it with the goal of being efficient.

		If the need to iterate over a tree emerges, we've built iterators for this job. There is no guarantee about the order of the returned elements, because the iterator is designed to return every element in the tree with only one pass. When iteration is over, the user will have to destroy the iterator (see appendix \ref{chap:doc}. Also, like in the list implementation we provide functions to destroy the tree, but the user must free each element stored in each node.

	\subsection{Macros}
		This section only describe two macros. The first one is a logging mechanism. We've developed a macro that logs messages with different kind of error levels and supports output formatting. This macro can be found	almost in any source file we've written. However, it is compilation dependent and when compiled without certain flags it won't get into the binary file (see appendix \ref{chap:gcc}). This helps us debug the code when developing and keeps the binary smaller when a final version is created.

		The second macro is an allocation macro. The simplest way to describe this macro is to say that its a one line code \emph{malloc} with type cast that exits on allocation failure. In other words, it allocates space and returns a pointer to it, casted to a specific type. If allocation fails, a log message is printed and the program will exit (see appendix \ref{chap:doc}).

%END Arch%

%Eval%
\chapter{Evaluation}\label{chap:eval}

\section{Test description}\label{sec:testdesc}
	This section presents some tests made to our library. We present only two kinds of tests, that were made in different kinds of systems.\\

	The first kind of test is done with a 131986 line XML with 12MB. We're parsing the XML and querying it for a total of 127266 elements. The test was performed on MS-Windows 7, Linux 2.6 and Mac OSx 10.4. Each
	operating system was running on a 2.6GHz 32-bit intel core duo with 1GB of RAM memory. We've also performed this test on an 250MHz Cavium ARM9 CPU, with 64MB of RAM memory. For each configuration, we've ran
	the test two times - One without querying engine optimization and other with querying engine optimization. \\

	The second test was done with the same configurations, the only difference is that the XML has a total of 36 lines, 0.77KB and we're querying it for 6 elements.\\

	Sections \ref{sec:mprof} and \ref{sec:tprof} show the results for these tests. It's important to know that every test was done with a cold cache.

\section{Profiling memory usage}\label{sec:mprof}
	This section shows the results for the memory usage on the different systems. The following table shows the result for the first kind of test. Each value is presented in Megabytes and represents the peak usage, that is the maximum amount of memory used.

	\begin{center}
  			\begin{tabular}{ | l | r | r | r | r | r | }
			    \hline
				       	       		     			 & Non-optimized (MB) & Optimized (MB)\\ \hline
				    MS-Windows 7   			&  41  & 41  \\ \hline
				    Linux 2.6 (Intel core duo) 	&  34  & 34  \\ \hline
				    Linux 2.6 (ARM cpu)   		&  37  & 37 \\ \hline
				    Mac OSx 10.4   			&  35  & 35 \\
			    \hline
			\end{tabular}
	\end{center}

	From this first test we can already conclude that optimizing the query engine didn't do nothing to optimize space. In fact, further analyzes to our profiling output reveals that the peak memory usage is related to the XML parser. At this point we can't do anything because the parser has already been optimized and the parsed data doesn't have such an overhead. However, with the flag \emph{USE\_DICT} (see appendix \ref{chap:gcc}) the	amount of memory usage drops about 6M.

	The next table shows the results for the second kind of test.

	\begin{center}
  			\begin{tabular}{ | l | r | r | r | r | r | }
			    \hline
				       	       		     			 & Non-optimized (MB) & Optimized (MB)\\ \hline
				    MS-Windows 7   			&  0.026  & 0.026  \\ \hline
				    Linux 2.6 (Intel core duo) 	&  0.019  & 0.019  \\ \hline
				    Linux 2.6 (ARM cpu)   		&  0.022  & 0.022 \\ \hline
				    Mac OSx 10.4   			&  0.020  & 0.020 \\
			    \hline
			\end{tabular}
	\end{center}

	This test confirms that the memory usage peak isn't related with the querying engine. Yet again, a detailed analysis of our profiling output revealed that the peak usage was when data was being parsed. The results from both tests show that the memory usage depends heavily on the size of the XML and not so much on the number of elements to be queried. We believe that most of XML documents used in Home Automation systems are small, thus not occupying great amounts of memory. That said, the results shows that our solution is adequate for such systems.

\section{Time usage}\label{sec:tprof}
	This section shows the time results for each kind of test. The value units are given in seconds. The first table shows the results for the first kind of test.

	\begin{center}
  			\begin{tabular}{ | l | r | r | r | r | r | }
			    \hline
				       	       		     			 & Non-optimized (s) & Optimized (s)\\ \hline
				    MS-Windows 7   			&  23.12  & 0.74  \\ \hline
				    Linux 2.6 (Intel core duo) 	&  18.82  & 0.67  \\ \hline
				    Linux 2.6 (ARM cpu)   		&  32.36  & 1.3 \\ \hline
				    Mac OSx 10.4   			&  21.78  & 0.71 \\
			    \hline
			\end{tabular}
	\end{center}

	This first test shows the impact of query plan evaluation in our design. When optimized the speedup gained is around:

		$$speedup = \frac{non-optimized}{optimized} = \frac{18.82}{0.67} = 28.09$$

	In fact, a detailed analysis of our profiling output showed that the non-optimized version wasted to much time on evaluating each query plan. For this reason we've optimized it and show impressive results. The following table shows the results for the second kind of test.

	\begin{center}
  			\begin{tabular}{ | l | r | r | r | r | r | }
			    \hline
				       	       		     			 & Non-optimized (s) & Optimized (s)\\ \hline
				    MS-Windows 7   			&  0.003  & $<$ 0.001  \\ \hline
				    Linux 2.6 (Intel core duo) 	&  0.002  & $<$ 0.001  \\ \hline
				    Linux 2.6 (ARM cpu)   		&  0.007  & $<$ 0.001 \\ \hline
				    Mac OSx 10.4   			&  0.002  & $<$ 0.001 \\
			    \hline
			\end{tabular}
	\end{center}

	This test confirms that the query engine optimization was successful. In fact, the optimized version is so fast that there isn't enough granularity to give the precise time it took the program to run.

	Another conclusion we take is that memory usage has little impact in time usage. In fact, our profiling output shows that the memory usage peak lasts only during parsing and them drops abruptly.
%END Eval%

%Conclusion%
\chapter{Conclusion} \label{chap:concl}
	We've presented a solution for an XML library designed for embedded systems. The main goal was to build it as light as possible, while giving the user a flexible and expandable way of parsing, querying and manipulating XML documents.

	We've built a library in three modules - A parser module; A DOM module; A querying engine module. Each has its own functionalities and can be modified almost effortlessly.

	We've shown that the performance depends heavily on the size of the XML being parsed. Our goal has been reach, as our library provides a flexible way to speedup development of applications that use XML documents
	to store configuration properties.

%END Conclusion%

\appendix
\chapter{Compiling and using the code}\label{chap:gcc}
	This section shows how to compile our library and compile other programs with our library.

	Compiling libxmlquery is very easy. The source code ships with a Makefile, so you just need to type in \emph{make}. On Unix based systems the \emph{make} command comes within the system, on MS-Windows we encourage the use of Mingw.

	When \emph{make} finishes you'll notice a file with the extension \textbf{.so} (Shared Object). You just need to pass this object to the compiler when linking and you're done. You have compiled your program against
	libxmlquery.

	As an example consider the command using \emph{gcc}:

	\begin{center}
	\lstset{language=bash,caption=Compiling a program against our library, captionpos=b}
	\begin{tabular}{c}
	\begin{lstlisting}
	gcc -o outputname libxmlquery.so myapplication.c
	\end{lstlisting}
	\end{tabular}
	\end{center}

	\section{Compilation flags}
		If the flag \emph{DEBUG} is passed to the compiler when compiling our library, then more debug output will be generated. As an example:

	\begin{center}
	\lstset{language=bash,caption=Compiling libxmlquery with debug output, captionpos=b}
	\begin{tabular}{c}
	\begin{lstlisting}
	gcc -DDEBUG -c <libxmlquery source files>
	\end{lstlisting}
	\end{tabular}
	\end{center}

		The flag \emph{USE\_DICT} optimizes memory usage in most of the cases. If used, a dictionary will be created and all node names, values, attributes will be stored in a global dictionary. Generaly, very big xml files have a great amount of repetition, by using a dictionary this repetition can be used to limit the amount of memory used to store the xml it self. However, if the XML is massively edited the dictionary will grow considerably and might become larger than the overhaed of the repetition. Another drawback is parsing speed, each string value in the xml will require $\Theta(\log_2(n))$ where $n$ is the size of the dictionary. Yet another problem is found when editing the xml nodes, inplace editing of string values would result in all nodes that share that string being edited. This flag provides a gain of around 20\% memory usage reduction in large files. As an example:

	\begin{center}
	\lstset{language=bash,caption=Compiling libxmlquery with dictionary optimization, captionpos=b}
	\begin{tabular}{c}
	\begin{lstlisting}
	gcc -DUSE_DICT -c <libxmlquery source files>
	\end{lstlisting}
	\end{tabular}
	\end{center}


\chapter{Sample applications}\label{chap:app} %lol%
	\section{Shell}
		The application shell is just a simple shell that enables loading XML documents and querying them. Its purpose is just to demonstrate the use of queries. Consider this typical use:

\begin{center}
\lstset{language=bash,caption=Typical usage of the shell application. The banner was intentionally removed., captionpos=b}
\begin{tabular}{c}
\begin{lstlisting}
>>> load ../test.xml doc
XML ../test.xml loaded and stored in doc
>>> query doc "@room device"
Node:
<device state="on" type="switch" />

Node:
<device state="87" type="blind" />

Node:
<device state="94" type="lights">
  <coord>
    <x>
      205
    </x>
    <y>
      97
    </y>
    <z>
      256
    </z>
  </coord>
</device>

>>> quit
exiting...
\end{lstlisting}
\end{tabular}
\end{center}

		In this example, the file \textbf{test.xml} gets loaded into the symbol \textbf{doc}. Then a query is performed on this symbol. It is possible to execute more commands. Type \emph{help} to get a description of them.

		The shell application comes with a Makefile, so it's easy to compile it. To compile in other systems see appendix \ref{chap:gcc}.

\chapter{Documentation}\label{chap:doc}
	This section provides all the documentation for the libxmlquery. The most recent version of the documentation is at \url{http://nullable.github.com/libxmlquery/}.

\end{document}

